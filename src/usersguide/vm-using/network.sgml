<section id="network" xreflabel="KVM Networking">
	<title>KVM Networking (Advanced)</title>

<para>
In order to support KVM Virtual Machines it is neccesary to have in place a 
particular network configuration on the hosting servers (which can be frontends
or vm-containers). In particular bridges must be set up to provide Virtual 
Machine with network connectivity (KVM networking differ substantially from 
XEN networking). In this paragraph we will explain in much details what are 
the differences from a standard Rocks Cluster network configuration.
</para>

<section>
<title>VM Network Bridging to Physical Devices</title>

<para>
When a VM is bridged to the physical device, it must be assigned in the same subnet as the physical device with 
a compatible IP address (this case is exactly what is described in <xref linkend="using-vm-physical-frontend"> ).
</para>

<para>
For this reason every Ethernet interfaces is renamed with a 'p' (for physical) in front of its 
original name. The interface is also put into promiscuous mode and then 
attached to a bridge named with the original name of the interface. The IP 
address and netmask are finally configured on the bridge device. 
</para>

<para>
For example if you have an eth0 which is supposed to be configured with IP
address 1.2.3.4 and netmask 255.255.255.0 you will end up with the physical 
interface named peth0 in promiscuous mode attached to a bridge named eth0 with 
IP address 1.2.3.4 and netmask 255.255.255.0. The creation of the bridge 
device, the renaming of the physical interface and the assignment of the IP 
address is all managed by Rocks through the Red Hat init scripts. This 
means that <filename>rocks sync host network</filename> creates the proper 
configuration files in <filename>/etc/sysconfig/network-scripts/</filename>.
</para>


<para>
If you start a virtual compute on a VM Container, with the VLAN 0 (the virtual 
machine belongs to the same cluster as the physical frontend), you will see a
virtual device belonging to the virtual machine attached to the eth0 bridge.
</para>

<screen>
[root@vm-container-0-0 ~]# brctl show
bridge name	bridge id		STP enabled	interfaces
eth0		8000.00144f80de00	no		peth0
							vnet0
virbr0		8000.5254007953c7	yes		virbr0-nic
</screen>

<para>
As you can see from the previous example bridge eth0 is connected to peth0 
(the physical interface) and vnet0 (the virtual interface of the virtual machine).
virbr0 is a default bridge started by libvirt which you can ignore.
</para>
</section>

<section>
<title>Logical VLAN Devices</title>
<para>
In this scenario, The guest (hosted-vm-0-0-0) and the host (vm-container-0-0) are not in the same logical
network (this is the scenario described in <xref linkend="provisioning-virtual-cluster">).
</para>

<para>
The virtual machine hosted-vm-0-0-0 will be connected to the network through a 
Logical VLAN (for more information consult 
<ulink url="http://www.rocksclusters.org/roll-documentation/base/6.0/x1051.html#NETWORKING-LOGICAL-VLAN">
the Base Roll documentation on networking</ulink>).
Logical Vlan interfaces don't need bridge device and they are dynamically loaded only when 
the virtual machine is started. They are called after the physical name interface 
simply adding the number of the VLAN (e.g. VLAN 2 on peth0 will be called peth0.2).
Virtual machines are attached to the VLAN interface using a 
<ulink url="http://virt.kernelnewbies.org/MacVTap">MacVTap</ulink> driver which does 
not require any bridging infrastructure. After you run a 
<filename>rocks start host vm hosted-vm-0-0-0</filename> if you go on the physical 
node you will see an interface called peth0.2 (the physical peth0 with the VLAN tag 
set to 2) and the macvtap0 which is the interface used by the virtual host.
</para>

</section>


</section>

